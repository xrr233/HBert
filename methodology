Methodology:
在本节，我们将会具体介绍HBert的模型架构。我们的核心想法是，我们平时看到的网页展现的有序，是因为除了网页中的纯文本之外，还有许多的HTML tag帮助网页展现出清晰的结构。因此，网页背后的HTML文本则是天生的附带结构信息的训练语料，而这些结构信息又能强化文本之间的联系。因此，我们的模型就是为了模拟这一树状结构，使得预训练模型可以学习到网页中富含的结构信息，并应用于下游任务尤其是IR任务中。为此，我们提出了模型HBert，其模型图如图所示。
我们的模型分为三个阶段，(1)预训练阶段 (2)上层模型微调阶段 (3)下游任务微调阶段。在第一个阶段中，我们采用了Bert的模型结构，并使用了Bert-base的参数配置。与其不同的是，我们采用的不是纯文本，而是网页中提取出的HTML文本，并只使用了MLM作为其训练的objective。在第二阶段，我们构造了一个分层的模型，其具体结构如图所示，我们通过模拟网页的DOM树的结构并构造上层任务，让纯文本模型学习到网页中文本的结构信息。在第三阶段，我们将对具体的下游任务进行微调。如果是纯文本任务，则只是用Text Encoder，而对于由HTML构成的数据集，我们则采用整个模型。接下来我们将具体的介绍这三个阶段的工作过程。
1.预训练阶段。我们首先对模型中的叶子节点部分模型作了预训练，使模型的叶子节点有编码HTML文本的能力，其训练语料使用的也是HTML文本。具体的说，我们沿用了Bert-Base的参数配置。由于HTML文本中有标签文本类似于<a>,<div>,<p>等，我们在词表中加入了所有的HTML标签。其次，标签与文本在语义表示的维度是不一样的，而同一个标签也分为起始标签和闭合标签，如<a>和<\a>,因此我们在bert结构中的positional embedding 和 segment embedding以外，我们还加入了tag embedding来标志当前token的种类。具体的说，就是对于一个token,如果它是一个普通文本token，则标记为0，如果是一个起始标签，则标记为1，是一个终止标签，则标志为2。最终输入模型的Embedding则为Word embedding + Positional embedding + Tag embedding。其次，为了保证HTML文本的连续性和完整性，我们从网页中抽取一条训练数据时并不是采用直接截断的方案，而是首先将一个网页的HTML展开成一个树状结构，并从中选取节点，抽取出其子HTML文本，作为一个训练语料，如果长度超过512，则不予采用。如图中的??节点,抽取出的一条训练数据则为<p> i want to go to sleep <\p>。我们使用MLM作为叶子节点预目训练阶段的优化标，按照15%的概率MASK掉该句话中的token，并预测原始token。可以预见的是，由于高层节点的全部HTML都较长，模型无法全部加载进去，因此模型对于高层节点的结构信息以及语义信息仍缺乏了解，因此我们设计了上层结构，微调下层模型，使其能更完整的衡量一个网页的完整信息。

2.上层模型微调阶段。
已有的利用预训练模型进行IR相关任务类似于文档排序，采用的大部分是直接使用Bert产生的CLS作Fine tune或是在纯文本任务上设计新的优化目标。然而这样的训练方式没有考虑到或是没有显性的考虑到一个网页中的层级以及结构信息，这主要是由于传统的预训练模型没有一个结构上的设计可以对网页的结构进行建模。在该阶段，我们采用了一个多层级的Bert来对HTML构成的DOM树建模。对于一个网页，我们首先将其构建成一个DOM树，树中的叶子节点为文本节点，非叶子节点为tag，我们通过使用多层级的Bert来模拟这种构造，其内部每一层由Text Encoder 和Node Encoder 组成，Text Encoder负责编码该层的文本节点，Node Encoder负责编码该层的叶子节点。由于计算资源的限制，我们采取了多种方案来减小模型的大小以及计算的开销。（1）由于每一层相同类型的Encoder是同样的功能，因此我们采用了所有层的Text encoder 共享参数，所有层的Node Encoder共享参数。（2）一个非叶子节点的孩子数目是有限的，大部分非叶子节点的数目只有个位数。其次，大部分有大量孩子节点的非叶子节点，其孩子节点倾向于同质化。因此我们将Text encoder的最大长度设置为256，Node Encoder的最大长度设置为10。（3）设置一个最大层深k,一个DOM树的前k-1层原样保留，k层以下的部分则采用子叶子节点HTML的方式，归并到底k层中。这样可以将一个模型的层数压缩到k层。在该阶段中，Text Encoder由1中预训练的模型初始化。其中Text Encoder负责编码文本，Node Encoder负责编码节点，如图所示，对叶子节点，其只需要通过1中训练的Text Encoder 进行编码，并将其CLS所产生的向量表示传入其父节点，作为父节点的一个token embedding。对于非叶子节点，其传入的向量由两部分组成，一部分是由上一层Text Encoder传入的Text表示，一部分是由上一层Node Encoder 传入的非叶子节点表示。在Node Encoder中，我们同样加入了positional embedding，因为孩子的相对顺序也是有意义的，如...。这样，一个HTML从叶子节点开始，将其语义信息一步步向上层传递，每经过一层则产生一个能代表该标签的向量表示，最终产生一个文档的向量表示。具体来说，在我们的模型中，每一个DOM树的节点可以通过其对应Encoder的CLS输出向量表示，因此对于一个非叶子节点，其n个孩子可以通过n个向量表示，我们通过替换其中一个向量构造正负例，使用该叶子节点的输出对其中是否有被替换过的向量做出预测。具体来说，我们从模型的第k层出发，由于这一层是最底层，所以只可能有Text节点。对于所有该层句子Sk = (sk1,sk2,...,skn)，ski = (wki1,wki2,...,wkit)，对于每个句子，我们使用textencoder编码，并取其[CLS]token的向量表示，作为该句话的表示，得到hki，即hki = CLS(textencoder(ski))。hk = (hk1,hk2,...,hkn)。我们将所有该部分向量传输到第k-1层。对于第j层(1<j<k)，其分为两部分，textencoder和nodeencoder，textencoder的过程同第k层，node encoder的编码过程如下：对第j层的p个非叶子节点(N1,N2,...,Np)，其中每一个非叶子节点由qi个孩子节点，其向量表示已经在第j+1层计算过，表示为(hj+11,hj+12,...,hj+1qi)，则每一个叶子节点可以表示为一个向量序列(Embedding(CLS),Embedding(Ni),hj+11,hj+12,...,hj+1qi)。将其输入Node Encoder，并进行self-attention操作，即。最终在第一层，其只有一个HTML标签，我们将该标签的CLS输出作为整个文档的向量表示。

有了模型的架构，我们还需要设计具体的任务，调整上层模型以及下层模型，使模型不管在纯文本任务或是带HTML的任务中都可以有良好的表现，我们因此设计了两个任务。

（1）Parallel Chirdren Modeling（PCM），
在我们的结构中，我们通过构造多层级的方式模拟了HTML构成的DOM树结构。而在DOM树组成的结构中，最明显的便是父子关系以及兄弟关系，因此我们首先对这一部分的信息进行建模。在我们的模型中，DOM树结构中的每一个节点都有一个向量表示，而同一个非叶子节点的所有输入就表示了该非叶子节点的所有孩子节点信息，这些孩子节点的高维向量表示应该是有丰富的联系的。 具体来说，我们在树中，随机挑选出一个非叶子节点，其组成序列输入node encoder前的表示为(Embedding(CLS),Embedding(Ni),hj+11,hj+12,...,hj+1qi)，我们将该向量序列作为正例，并从同一个训练数据中，随机筛选非该节点孩子节点的一个输出向量，换掉该序列中的一个，变为(Embedding(CLS),Embedding(Ni),wrong,hj+12,...,hj+1qi)，作为负例。其次，我们需要强调的是，该任务不仅仅可以同时考虑到兄弟节点之间的联系，还可以同时建模父子节点之间的联系。具体来说，建模父子关系一个直观想法就是，Mask掉一个孩子节点，使用父亲节点的输出与该孩子节点的输出做一个训练任务，然而父亲节点的输出来自于其所有孩子节点(通过self-attention)，可以将父亲节点的输出看做被MASK掉的节点的所有兄弟节点的集合，其与该孩子节点的输出作辅助任务也相当于该孩子节点与其所有兄弟节点一起作辅助任务，这就与我们实现PCM的方式达成一致。 我们使用pair-wise loss作为优化指标，通过CLS位置输出的向量进行预测。
（2）Tag Prediction(TP),我们知道，在HTML文本中，tag是有意义的，如在<a>标签下的文本是一个超链接，<title>下的文本更可能表达整个文档的主旨，因此我们希望同时构造一个tag级别的MLM任务，希望模型可以通过一个节点的向量输出，根据其附带的高维信息，推断出其原始所在的标签是哪一个。具体来说，在一个非叶子节点上，其输入的向量表示为(Embedding(CLS),Embedding(Ni),hj+11,hj+12,...,hj+1qi)，我们将其表示tag的位置MASK掉，变为(Embedding(CLS),Embedding([MASK]),hj+11,hj+12,...,hj+1qi),我们将其在MASK位置上的输出取出来，通过一个MLP，预测原始tag，表示如下。
（3）Masked lauguage model(MLM)，我们在进行使用上层模型微调的同时，同时也需要保持下层模型的有效性，因此我们在叶子节点的Text Encoder处仍然加入了MLM loss，其实现过程与1中相同。最终其任务损失为

我们将这三部分损失累加在一起作为模型的整体损失，通过MLM任务保持下层编码HTML文本的有效性，并通过上层的PCM以及TP任务对下层的Text Encoder 进行微调，以期望Text Encoder可以捕捉到文本间的结构信息，在IR任务上能有更好的效果。


（3）下游任务微调阶段
在之前的下层模型预训练阶段，我们训练了一个以HTML文本作为训练数据的Bert，使该模型有编码HTML文本的能力。在之后的上层模型微调阶段，我们使用该Bert初始化了模型的Text encoder，并采用了多层级的方式模拟HTML转化成的DOM树结构，并在上层设计任务使得Text Encoder具有获得文本之间结构信息的能力。而在下游任务中，对于纯文本任务，我们将使用HBert Fine tune过后的Text Encoder，其模型结构与Bert相同，只需要将文本的tag embedding全置为0即可。由于我们在训练中没有原始Bert的NSP任务，并没有训练[SEP]这个特殊token，因此我们采用了IR中retrieval阶段经常使用的双塔模型，而近来效果最好的双塔模型莫过于Colbert,我们使用训练以及fine tune过的Text encoder作为Colbert的初始化模型，并在IR下游任务上进行fine tune，即对于一个query和document，q=Colbert(query),d=ColBert(document)，score=q*d。使用该score表示最终的计算排名分数。
